{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:37:59.056674Z",
     "iopub.status.busy": "2021-01-11T14:37:59.056241Z",
     "iopub.status.idle": "2021-01-11T14:38:02.220365Z",
     "shell.execute_reply": "2021-01-11T14:38:02.220770Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    return 1 - np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = np.sum(y_true * y_pred) / np.sum(y_pred)\n",
    "    recall = np.sum(y_true * y_pred) / np.sum(y_true)\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training and test set, and build vocabulary using only words from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.223317Z",
     "iopub.status.busy": "2021-01-11T14:38:02.222964Z",
     "iopub.status.idle": "2021-01-11T14:38:02.224598Z",
     "shell.execute_reply": "2021-01-11T14:38:02.224249Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_files = glob.glob(\"txt_sentoken/neg/*.txt\")\n",
    "pos_files = glob.glob(\"txt_sentoken/pos/*.txt\")\n",
    "files = neg_files + pos_files\n",
    "\n",
    "n = len(files)\n",
    "n_test = 400\n",
    "n_train = n - n_test\n",
    "\n",
    "test_idx = np.random.choice(n, size=n_test, replace = False)\n",
    "train_idx = np.delete(np.arange(n), test_idx)\n",
    "\n",
    "vocab = {}\n",
    "index = 0\n",
    "for file in np.array(files)[train_idx]:\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "            \n",
    "pickle.dump(vocab, open(\"vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word count feature vectors $x_i$ for each document, i.e. $x_{ij} = $ the number of instances of word $j$ in document $i$. Generate labels $y_i$, which equal $0$ for negative documents and $1$ for positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.228124Z",
     "iopub.status.busy": "2021-01-11T14:38:02.227760Z",
     "iopub.status.idle": "2021-01-11T14:38:02.228458Z",
     "shell.execute_reply": "2021-01-11T14:38:02.228781Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "X_train = np.zeros((n_train, vocab_size), dtype=np.int8)\n",
    "X_test = np.zeros((n_test, vocab_size), dtype=np.int8)\n",
    "\n",
    "# Later indices are positive (1) and earlier are negative (0).\n",
    "y_train = (train_idx >= 1000).astype(int)\n",
    "y_test = (test_idx >= 1000).astype(int)\n",
    "\n",
    "for i in range(n_train):\n",
    "    file = files[train_idx[i]]\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        X_train[i, vocab[token]] += 1\n",
    "\n",
    "for i in range(n_test):\n",
    "    file = files[test_idx[i]]\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        # Ignore unknown tokens.\n",
    "        if token in vocab:\n",
    "            X_test[i, vocab[token]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.232171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.231797Z",
     "iopub.status.idle": "2021-01-11T14:38:02.855965Z",
     "shell.execute_reply": "2021-01-11T14:38:02.855580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1600\n",
      "Test size: 400\n",
      "Vocabulary size: 45618\n",
      "\n",
      "X_train shape: (1600, 45618)\n",
      "X_test shape: (400, 45618)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", n_train)\n",
    "print(\"Test size:\", n_test)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print()\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.865171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.863077Z",
     "iopub.status.idle": "2021-01-11T14:38:02.866412Z",
     "shell.execute_reply": "2021-01-11T14:38:02.866744Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressor():\n",
    "    \n",
    "    def __init__(self, lambda_, batch_size, epochs, lr, verbose):\n",
    "        self.lambda_ = lambda_\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Run gradient descent to fit cross entropy loss objective.\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        if not hasattr(self, 'weights'):\n",
    "            self.weights = np.random.normal(size=d+1)\n",
    "        checkpoint = self.epochs // 10\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            order = np.random.permutation(n)\n",
    "            num_batch = n // self.batch_size\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                indices = order[i:min(i + batch_size, n)]\n",
    "                X_batch = X_appended[indices]\n",
    "                y_batch = y[indices]\n",
    "                \n",
    "                self.gradient_step(X_batch, y_batch)\n",
    "                \n",
    "            # Compute training loss.\n",
    "            if verbose and (epoch % checkpoint == 0):\n",
    "                logits = np.dot(X_appended, self.weights)\n",
    "                probs = self.sigmoid(logits)\n",
    "                loss = (-(y * np.log(probs) + (1 - y) * np.log(1 - probs)).sum() + 0.5 * self.lambda_ * np.dot(self.weights, self.weights)) / n\n",
    "                \n",
    "                y_pred = (1 + np.sign(logits)) / 2\n",
    "                train_accuracy = accuracy_score(y, y_pred)\n",
    "                print(\"Epoch %d \\t cross entropy loss: %0.4f train accuracy: %0.3f\" % (epoch, loss, train_accuracy))\n",
    "            \n",
    "                \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict 1 for positive sentiment and 0 for negative.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        \n",
    "        return (1 + np.sign(np.dot(X_appended, self.weights))) / 2\n",
    "    \n",
    "    def gradient_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute gradient and perform one optimization step.\n",
    "        \"\"\"\n",
    "        # Use l1 regularization.\n",
    "        \n",
    "        n = len(X)\n",
    "        probs = self.sigmoid(np.dot(X, self.weights))\n",
    "        grad = (np.dot(X.T, (probs - y)) + self.lambda_ * self.weights) / n\n",
    "        self.weights = self.weights - self.lr * grad\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        Z_clipped = np.clip(Z, -10, 10)\n",
    "        return 1 / (1 + np.exp(-Z_clipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the logistic regression code works on a toy example: class-conditional Gaussians, centered at $-\\mu$ for the negative class and $+\\mu$ for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:32.929921Z",
     "iopub.status.busy": "2021-01-11T15:46:32.929571Z",
     "iopub.status.idle": "2021-01-11T15:46:33.145605Z",
     "shell.execute_reply": "2021-01-11T15:46:33.145252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 0.6838 train accuracy: 0.700\n",
      "Epoch 1 \t cross entropy loss: 0.2298 train accuracy: 0.900\n",
      "Epoch 2 \t cross entropy loss: 0.1388 train accuracy: 0.932\n",
      "Epoch 3 \t cross entropy loss: 0.0864 train accuracy: 0.961\n",
      "Epoch 4 \t cross entropy loss: 0.0640 train accuracy: 0.975\n",
      "Epoch 5 \t cross entropy loss: 0.0477 train accuracy: 0.986\n",
      "Epoch 6 \t cross entropy loss: 0.0431 train accuracy: 0.986\n",
      "Epoch 7 \t cross entropy loss: 0.0381 train accuracy: 0.991\n",
      "Epoch 8 \t cross entropy loss: 0.0362 train accuracy: 0.991\n",
      "Epoch 9 \t cross entropy loss: 0.0319 train accuracy: 0.993\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "n_tr = 1000\n",
    "n_te = 100\n",
    "d = 10\n",
    "\n",
    "mu = np.ones(d)\n",
    "cov = np.eye(d)\n",
    "\n",
    "X_tr = np.concatenate(\n",
    "    [\n",
    "        np.random.multivariate_normal(-mu, cov, size = n_tr // 2),\n",
    "        np.random.multivariate_normal(mu, cov, size = n_tr // 2)\n",
    "    ]\n",
    ")\n",
    "y_tr = np.concatenate(\n",
    "    [\n",
    "        np.repeat(0, n_tr // 2),\n",
    "        np.repeat(1, n_tr // 2)\n",
    "    ]\n",
    ")\n",
    "X_te = np.concatenate(\n",
    "    [\n",
    "        np.random.multivariate_normal(-mu, cov, size = n_te // 2),\n",
    "        np.random.multivariate_normal(mu, cov, size = n_te // 2)\n",
    "    ]\n",
    ")\n",
    "y_te = np.concatenate(\n",
    "    [\n",
    "        np.repeat(0, n_te // 2),\n",
    "        np.repeat(1, n_te // 2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "verbose = True\n",
    "lr = 0.03\n",
    "lambda_ = 0.001\n",
    "\n",
    "lr = LogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "lr.fit(X_tr, y_tr)\n",
    "y_pred = lr.predict(X_te)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Pang and Lee movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:33.149677Z",
     "iopub.status.busy": "2021-01-11T15:46:33.149334Z",
     "iopub.status.idle": "2021-01-11T15:46:33.228690Z",
     "shell.execute_reply": "2021-01-11T15:46:33.228342Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 200\n",
    "verbose = True\n",
    "lr = 0.01\n",
    "lambda_ = 0\n",
    "\n",
    "lr = LogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 4.2158 train accuracy: 0.506\n",
      "Epoch 20 \t cross entropy loss: 3.0453 train accuracy: 0.628\n",
      "Epoch 40 \t cross entropy loss: 2.3171 train accuracy: 0.687\n",
      "Epoch 60 \t cross entropy loss: 1.9340 train accuracy: 0.734\n",
      "Epoch 80 \t cross entropy loss: 1.6909 train accuracy: 0.756\n",
      "Epoch 100 \t cross entropy loss: 1.3230 train accuracy: 0.806\n",
      "Epoch 120 \t cross entropy loss: 1.1458 train accuracy: 0.820\n",
      "Epoch 140 \t cross entropy loss: 0.9431 train accuracy: 0.834\n",
      "Epoch 160 \t cross entropy loss: 0.8340 train accuracy: 0.856\n",
      "Epoch 180 \t cross entropy loss: 1.4000 train accuracy: 0.787\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lr.fit(X_train, y_train)\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.745\n",
      "F1 score: 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Lexicon-Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect positive and negative sentiment words, and discard those that are not in the vocabulary of the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of negative words: 4814\n",
      "Negative words in vocab: 2974\n",
      "Original number of positive words: 2036\n",
      "Positive words in vocab: 1438\n"
     ]
    }
   ],
   "source": [
    "neg_word_file = \"opinion-lexicon-English/negative-words.txt\"\n",
    "pos_word_file = \"opinion-lexicon-English/positive-words.txt\"\n",
    "\n",
    "vocab = pickle.load(open(\"vocab.pkl\", \"rb\"))\n",
    "\n",
    "with open(neg_word_file, encoding=\"ISO-8859-1\") as f:\n",
    "    neg_words_orig = f.readlines()\n",
    "print(\"Original number of negative words:\", len(neg_words_orig))\n",
    "neg_words = []\n",
    "for word in neg_words_orig[31:]:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        neg_words.append(candidate)\n",
    "\n",
    "print(\"Negative words in vocab:\", len(neg_words))\n",
    "            \n",
    "with open(pos_word_file, encoding=\"ISO-8859-1\") as f:\n",
    "    pos_words_orig = f.readlines()\n",
    "print(\"Original number of positive words:\", len(pos_words_orig))\n",
    "pos_words = []\n",
    "for word in pos_words_orig[31:]:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        pos_words.append(candidate)\n",
    "\n",
    "print(\"Positive words in vocab:\", len(pos_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess accuracy on test set, as training set was used at least to generate vocabulary. Subtract \"points\" from the prediction for the counts all negative words in the document, and add \"points\" for the counts of all positive words. Predict $1$ for positive score, and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "n, d = X_test.shape\n",
    "\n",
    "scores = np.zeros(n)\n",
    "\n",
    "for word in neg_words:\n",
    "    scores -= X_test[:, vocab[word]]\n",
    "        \n",
    "for word in pos_words:\n",
    "    scores += X_test[:, vocab[word]]\n",
    "    \n",
    "y_pred = (scores > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6775\n",
      "F1 score: 0.6783042394014962\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
