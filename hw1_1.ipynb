{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:41.140970Z",
     "iopub.status.busy": "2021-01-11T13:24:41.140304Z",
     "iopub.status.idle": "2021-01-11T13:24:42.074625Z",
     "shell.execute_reply": "2021-01-11T13:24:42.075074Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:42.079960Z",
     "iopub.status.busy": "2021-01-11T13:24:42.079465Z",
     "iopub.status.idle": "2021-01-11T13:24:42.818248Z",
     "shell.execute_reply": "2021-01-11T13:24:42.818679Z"
    }
   },
   "outputs": [],
   "source": [
    "# neg_files = glob.glob(\"txt_sentoken/neg/*.txt\")\n",
    "# pos_files = glob.glob(\"txt_sentoken/neg/*.txt\")\n",
    "# files = neg_files + pos_files\n",
    "# vocab = {}\n",
    "# index = 0\n",
    "# for file in files:\n",
    "#     with open(file) as f:\n",
    "#         raw_txt = f.read()\n",
    "#     txt_arr = raw_txt.split()\n",
    "#     for token in txt_arr:\n",
    "#         if token not in vocab:\n",
    "#             vocab[token] = index\n",
    "#             index += 1\n",
    "            \n",
    "# pickle.dump(vocab, open(\"vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word count feature vectors for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:42.825649Z",
     "iopub.status.busy": "2021-01-11T13:24:42.825096Z",
     "iopub.status.idle": "2021-01-11T13:24:47.982891Z",
     "shell.execute_reply": "2021-01-11T13:24:47.983443Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f12d0de0ea3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# vocab_size = len(vocab)\n",
    "# n = len(files)\n",
    "# n_neg = len(neg_files)\n",
    "# n_pos = len(pos_files)\n",
    "\n",
    "# X = np.zeros((n, vocab_size), dtype=np.int8)\n",
    "# y = np.concatenate((np.repeat(0, n_neg), np.repeat(1, n_pos))).astype(np.int8)\n",
    "\n",
    "# for i, file in enumerate(files):\n",
    "#     with open(file) as f:\n",
    "#         raw_txt = f.read()\n",
    "#     txt_arr = raw_txt.split()\n",
    "#     for token in txt_arr:\n",
    "#         X[i, vocab[token]] += 1\n",
    "        \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=400)\n",
    "\n",
    "# pickle.dump(X_train, open(\"X_train.pkl\", \"wb\"))\n",
    "# pickle.dump(X_test, open(\"X_test.pkl\", \"wb\"))\n",
    "# pickle.dump(y_train, open(\"y_train.pkl\", \"wb\"))\n",
    "# pickle.dump(y_test, open(\"y_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:47.988825Z",
     "iopub.status.busy": "2021-01-11T13:24:47.988291Z",
     "iopub.status.idle": "2021-01-11T13:24:48.616604Z",
     "shell.execute_reply": "2021-01-11T13:24:48.616997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1600\n",
      "Test size: 400\n",
      "Vocabulary size: 34542\n",
      "\n",
      "X_train shape: (1600, 34542)\n",
      "X_test shape: (400, 34542)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open(\"X_train.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"X_test.pkl\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y_train.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"y_test.pkl\", \"rb\"))\n",
    "\n",
    "vocab = pickle.load(open(\"vocab.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print()\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Multinomial Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:48.663172Z",
     "iopub.status.busy": "2021-01-11T13:24:48.662603Z",
     "iopub.status.idle": "2021-01-11T13:24:48.664566Z",
     "shell.execute_reply": "2021-01-11T13:24:48.664956Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegressor():\n",
    "    \n",
    "    def __init__(self, lambda_, batch_size, epochs, lr, verbose):\n",
    "        self.lambda_ = lambda_\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n, d = X.shape\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        self.weights = np.random.normal(size=d+1)\n",
    "        checkpoint = epochs // 10\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            order = np.random.permutation(n)\n",
    "            num_batch = n // batch_size\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                indices = order[i:min(i + batch_size, n)]\n",
    "                X_batch = X_appended[indices]\n",
    "                y_batch = y[indices]\n",
    "                \n",
    "                self.gradient_step(X_batch, y_batch)\n",
    "                \n",
    "            # Compute training loss.\n",
    "            if verbose and (epoch % checkpoint == 0):\n",
    "                logits = np.dot(X_appended, self.weights)\n",
    "                probs = self.sigmoid(logits)\n",
    "                loss = -(y * np.log(probs) + (1 - y) * np.log(1 - probs)).sum() / n + 0.5 * self.lambda_ * np.dot(self.weights, self.weights)\n",
    "                \n",
    "                y_pred = (1 + np.sign(logits)) / 2\n",
    "                train_error = np.mean(np.abs(y - y_pred))\n",
    "                print(\"Epoch %d \\t cross entropy loss: %0.4f train error %0.3f\" % (epoch, loss, train_error))\n",
    "            \n",
    "                \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if not hasattr(self, 'fitted'):\n",
    "            raise NotFittedError(\"This MultinomialLogisticRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\")\n",
    "            \n",
    "        n = len(X)\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        \n",
    "        return (1 + np.sign(np.dot(X_appended, self.weights))) / 2\n",
    "    \n",
    "    def gradient_step(self, X, y):\n",
    "        \n",
    "        n = len(X)\n",
    "        probs = self.sigmoid(np.dot(X, self.weights))\n",
    "        grad = (np.dot(X.T, (probs - y)) + self.lambda_ * self.weights) / n\n",
    "        self.weights = self.weights - self.lr * grad\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        Z_clipped = np.clip(Z, -10, 10)\n",
    "        return 1 / (1 + np.exp(-Z_clipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:24:48.671999Z",
     "iopub.status.busy": "2021-01-11T13:24:48.671484Z",
     "iopub.status.idle": "2021-01-11T13:38:12.132131Z",
     "shell.execute_reply": "2021-01-11T13:38:12.132661Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cross_val = 100\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "verbose = False\n",
    "\n",
    "lambdas = np.power(10, np.random.uniform(low=-5, high=0.5, size=num_cross_val))\n",
    "lrs = np.power(10, np.random.uniform(low=-5, high=0.5, size=num_cross_val))\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "best_acc = 0\n",
    "best_lambda = 0\n",
    "best_lr = 0\n",
    "\n",
    "hyperparams = []\n",
    "\n",
    "print('Iter \\t lambda \\t lr \\t\\t train acc \\t val acc \\t val f1 ')\n",
    "for i in range(num_cross_val):\n",
    "    lambda_ = lambdas[i]\n",
    "    lr = lrs[i]\n",
    "    \n",
    "    mlr = MultinomialLogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "    mlr.fit(X_tr, y_tr)\n",
    "    y_pred = mlr.predict(X_val)\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    train_acc = accuracy_score(y_tr, mlr.predict(X_tr))\n",
    "        \n",
    "    hyperparams.append(\n",
    "        {\n",
    "            'lambda' : lambda_,\n",
    "            'lr' : lr,\n",
    "            'train_acc' : train_acc,\n",
    "            'val_acc' : val_acc,\n",
    "            'f1' : f1\n",
    "        }\n",
    "    )    \n",
    "    print('%d \\t %0.5f \\t %0.5f \\t %0.2f \\t\\t %0.2f \\t\\t %0.2f' % (i, lambda_, lr, train_acc, val_acc, f1))\n",
    "        \n",
    "pickle.dump(hyperparams, open(\"hyperparams.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lambda        lr  train_acc   val_acc        f1\n",
      "0   0.000035  0.000269   0.543750  0.418750  0.449704\n",
      "1   0.000199  0.000021   0.514844  0.496875  0.519403\n",
      "2   0.000055  0.008396   0.632031  0.259375  0.354223\n",
      "3   0.000372  0.028210   0.562500  0.365625  0.028708\n",
      "4   0.746300  2.104295   0.487500  0.490625  0.000000\n",
      "..       ...       ...        ...       ...       ...\n",
      "95  0.002314  0.000045   0.507812  0.484375  0.535211\n",
      "96  0.070781  0.050187   0.553125  0.428125  0.594235\n",
      "97  0.000199  0.000172   0.534375  0.456250  0.452830\n",
      "98  2.609784  0.000964   0.532813  0.396875  0.030151\n",
      "99  0.283345  0.057510   0.529687  0.465625  0.633833\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "hyperparams = pd.DataFrame(pickle.load(open(\"hyperparams.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>lr</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.718564</td>\n",
       "      <td>0.058442</td>\n",
       "      <td>0.513281</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.673597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.011996</td>\n",
       "      <td>1.293417</td>\n",
       "      <td>0.513281</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.673597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.701879</td>\n",
       "      <td>0.165927</td>\n",
       "      <td>0.513281</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.673597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.450486</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.555241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.021464</td>\n",
       "      <td>1.170003</td>\n",
       "      <td>0.514062</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>0.672199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lambda        lr  train_acc   val_acc        f1\n",
       "64  0.718564  0.058442   0.513281  0.509375  0.673597\n",
       "68  0.011996  1.293417   0.513281  0.509375  0.673597\n",
       "90  1.701879  0.165927   0.513281  0.509375  0.673597\n",
       "37  0.450486  0.000016   0.500000  0.509375  0.555241\n",
       "19  0.021464  1.170003   0.514062  0.506250  0.672199"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams.sort_values('val_acc', inplace=True, ascending=False)\n",
    "hyperparams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the code works on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 0.4498\n",
      "Epoch 10 \t cross entropy loss: 0.0609\n",
      "Epoch 20 \t cross entropy loss: 0.0374\n",
      "Epoch 30 \t cross entropy loss: 0.0281\n",
      "Epoch 40 \t cross entropy loss: 0.0220\n",
      "Epoch 50 \t cross entropy loss: 0.0195\n",
      "Epoch 60 \t cross entropy loss: 0.0183\n",
      "Epoch 70 \t cross entropy loss: 0.0173\n",
      "Epoch 80 \t cross entropy loss: 0.0168\n",
      "Epoch 90 \t cross entropy loss: 0.0164\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "d = 10\n",
    "\n",
    "mu = np.ones(d)\n",
    "cov = np.eye(d)\n",
    "\n",
    "X_neg = np.random.multivariate_normal(-mu, cov, size = n // 2)\n",
    "y_neg = np.repeat(0, n // 2)\n",
    "X_pos = np.random.multivariate_normal(mu, cov, size = n // 2)\n",
    "y_pos = np.repeat(1, n // 2)\n",
    "\n",
    "X = np.concatenate((X_neg, X_pos))\n",
    "y = np.concatenate((y_neg, y_pos))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "verbose = True\n",
    "lr = 0.001\n",
    "lambda_ = 0.001\n",
    "\n",
    "mlr = MultinomialLogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "mlr.fit(X_train, y_train)\n",
    "y_pred = mlr.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with better hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 17354.1443 train error 0.497\n",
      "Epoch 30 \t cross entropy loss: 14336.7710 train error 0.491\n",
      "Epoch 60 \t cross entropy loss: 11853.7137 train error 0.473\n",
      "Epoch 90 \t cross entropy loss: 9807.6763 train error 0.458\n",
      "Epoch 120 \t cross entropy loss: 8117.4998 train error 0.460\n",
      "Epoch 150 \t cross entropy loss: 6721.5702 train error 0.442\n",
      "Epoch 180 \t cross entropy loss: 5569.0041 train error 0.459\n",
      "Epoch 210 \t cross entropy loss: 4615.1826 train error 0.456\n",
      "Epoch 240 \t cross entropy loss: 3825.6719 train error 0.444\n",
      "Epoch 270 \t cross entropy loss: 3172.8950 train error 0.427\n",
      "Validation accuracy:  0.38125\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 300\n",
    "verbose = True\n",
    "lr = 0.01\n",
    "lambda_ = 1\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "try:\n",
    "    mlr = MultinomialLogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "    mlr.fit(X_tr, y_tr)\n",
    "    y_pred = mlr.predict(X_val)\n",
    "\n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "    print(\"Validation accuracy: \", val_acc)\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mlr, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.4525\n",
      "Test f1:  0.6191304347826087\n"
     ]
    }
   ],
   "source": [
    "mlr = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "y_pred = mlr.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test accuracy: \", test_acc)\n",
    "print(\"Test f1: \", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
