{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:37:59.056674Z",
     "iopub.status.busy": "2021-01-11T14:37:59.056241Z",
     "iopub.status.idle": "2021-01-11T14:38:02.220365Z",
     "shell.execute_reply": "2021-01-11T14:38:02.220770Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    return 1 - np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = np.sum(y_true * y_pred) / np.sum(y_pred)\n",
    "    recall = np.sum(y_true * y_pred) / np.sum(y_true)\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training and test set, and build vocabulary using only words from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.223317Z",
     "iopub.status.busy": "2021-01-11T14:38:02.222964Z",
     "iopub.status.idle": "2021-01-11T14:38:02.224598Z",
     "shell.execute_reply": "2021-01-11T14:38:02.224249Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_files = glob.glob(\"review_polarity/txt_sentoken/neg/*.txt\")\n",
    "pos_files = glob.glob(\"review_polarity/txt_sentoken/pos/*.txt\")\n",
    "files = neg_files + pos_files\n",
    "\n",
    "n = len(files)\n",
    "n_test = 400\n",
    "n_train = n - n_test\n",
    "\n",
    "np.random.seed(517)\n",
    "test_idx = np.random.choice(n, size=n_test, replace = False)\n",
    "train_idx = np.delete(np.arange(n), test_idx)\n",
    "\n",
    "vocab = {}\n",
    "index = 0\n",
    "for file in np.array(files)[train_idx]:\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "            \n",
    "pickle.dump(vocab, open(\"vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word count feature vectors $x_i$ for each document, i.e. $x_{ij} = $ the number of instances of word $j$ in document $i$. Generate labels $y_i$, which equal $0$ for negative documents and $1$ for positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.228124Z",
     "iopub.status.busy": "2021-01-11T14:38:02.227760Z",
     "iopub.status.idle": "2021-01-11T14:38:02.228458Z",
     "shell.execute_reply": "2021-01-11T14:38:02.228781Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "X_train = np.zeros((n_train, vocab_size), dtype=np.int8)\n",
    "X_test = np.zeros((n_test, vocab_size), dtype=np.int8)\n",
    "\n",
    "# Later indices are positive (1) and earlier are negative (0).\n",
    "y_train = (train_idx >= 1000).astype(int)\n",
    "y_test = (test_idx >= 1000).astype(int)\n",
    "\n",
    "for i in range(n_train):\n",
    "    file = files[train_idx[i]]\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        X_train[i, vocab[token]] += 1\n",
    "\n",
    "for i in range(n_test):\n",
    "    file = files[test_idx[i]]\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        # Ignore unknown tokens.\n",
    "        if token in vocab:\n",
    "            X_test[i, vocab[token]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.232171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.231797Z",
     "iopub.status.idle": "2021-01-11T14:38:02.855965Z",
     "shell.execute_reply": "2021-01-11T14:38:02.855580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1600\n",
      "Test size: 400\n",
      "Vocabulary size: 46182\n",
      "\n",
      "X_train shape: (1600, 46182)\n",
      "X_test shape: (400, 46182)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", n_train)\n",
    "print(\"Test size:\", n_test)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print()\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.865171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.863077Z",
     "iopub.status.idle": "2021-01-11T14:38:02.866412Z",
     "shell.execute_reply": "2021-01-11T14:38:02.866744Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressor():\n",
    "    \n",
    "    def __init__(self, lambda_, batch_size, epochs, lr, verbose):\n",
    "        self.lambda_ = lambda_\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Run mini-batch stochastic gradient descent to fit cross entropy loss objective.\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        if not hasattr(self, 'weights'):\n",
    "            self.weights = np.random.normal(size=d+1)\n",
    "        checkpoint = self.epochs // 10\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            order = np.random.permutation(n)\n",
    "            num_batch = n // self.batch_size\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                indices = order[i:min(i + batch_size, n)]\n",
    "                X_batch = X_appended[indices]\n",
    "                y_batch = y[indices]\n",
    "                \n",
    "                self.gradient_step(X_batch, y_batch)\n",
    "                \n",
    "            # Compute training loss.\n",
    "            if verbose and (epoch % checkpoint == 0):\n",
    "                logits = np.dot(X_appended, self.weights)\n",
    "                probs = self.sigmoid(logits)\n",
    "                loss = (-(y * np.log(probs) + (1 - y) * np.log(1 - probs)).sum() + 0.5 * self.lambda_ * np.dot(self.weights, self.weights)) / n\n",
    "                \n",
    "                y_pred = (1 + np.sign(logits)) / 2\n",
    "                train_accuracy = accuracy_score(y, y_pred)\n",
    "                print(\"Epoch %d \\t cross entropy loss: %0.4f train accuracy: %0.3f\" % (epoch, loss, train_accuracy))\n",
    "            \n",
    "                \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict 1 for positive sentiment and 0 for negative.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        \n",
    "        return (1 + np.sign(np.dot(X_appended, self.weights))) / 2\n",
    "    \n",
    "    def gradient_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute gradient and perform one optimization step.\n",
    "        \"\"\"\n",
    "        # Use l1 regularization.\n",
    "        \n",
    "        n = len(X)\n",
    "        probs = self.sigmoid(np.dot(X, self.weights))\n",
    "        grad = (np.dot(X.T, (probs - y)) + self.lambda_ * self.weights) / n\n",
    "        self.weights = self.weights - self.lr * grad\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        Z_clipped = np.clip(Z, -10, 10)\n",
    "        return 1 / (1 + np.exp(-Z_clipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the logistic regression code works on a toy example: class-conditional Gaussians, centered at $-\\mu$ for the negative class and $+\\mu$ for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:32.929921Z",
     "iopub.status.busy": "2021-01-11T15:46:32.929571Z",
     "iopub.status.idle": "2021-01-11T15:46:33.145605Z",
     "shell.execute_reply": "2021-01-11T15:46:33.145252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 0.2451 train accuracy: 0.908\n",
      "Epoch 1 \t cross entropy loss: 0.1820 train accuracy: 0.934\n",
      "Epoch 2 \t cross entropy loss: 0.1231 train accuracy: 0.953\n",
      "Epoch 3 \t cross entropy loss: 0.1030 train accuracy: 0.958\n",
      "Epoch 4 \t cross entropy loss: 0.0928 train accuracy: 0.963\n",
      "Epoch 5 \t cross entropy loss: 0.0737 train accuracy: 0.970\n",
      "Epoch 6 \t cross entropy loss: 0.0628 train accuracy: 0.978\n",
      "Epoch 7 \t cross entropy loss: 0.0552 train accuracy: 0.979\n",
      "Epoch 8 \t cross entropy loss: 0.0477 train accuracy: 0.984\n",
      "Epoch 9 \t cross entropy loss: 0.0379 train accuracy: 0.986\n",
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "n_tr = 1000\n",
    "n_te = 100\n",
    "d = 10\n",
    "\n",
    "mu = np.ones(d)\n",
    "cov = np.eye(d)\n",
    "\n",
    "X_tr = np.concatenate(\n",
    "    [\n",
    "        np.random.multivariate_normal(-mu, cov, size = n_tr // 2),\n",
    "        np.random.multivariate_normal(mu, cov, size = n_tr // 2)\n",
    "    ]\n",
    ")\n",
    "y_tr = np.concatenate(\n",
    "    [\n",
    "        np.repeat(0, n_tr // 2),\n",
    "        np.repeat(1, n_tr // 2)\n",
    "    ]\n",
    ")\n",
    "X_te = np.concatenate(\n",
    "    [\n",
    "        np.random.multivariate_normal(-mu, cov, size = n_te // 2),\n",
    "        np.random.multivariate_normal(mu, cov, size = n_te // 2)\n",
    "    ]\n",
    ")\n",
    "y_te = np.concatenate(\n",
    "    [\n",
    "        np.repeat(0, n_te // 2),\n",
    "        np.repeat(1, n_te // 2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "verbose = True\n",
    "lr = 0.03\n",
    "lambda_ = 0.001\n",
    "\n",
    "lr = LogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "lr.fit(X_tr, y_tr)\n",
    "y_pred = lr.predict(X_te)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Pang and Lee movie review data. Hyperparameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:33.149677Z",
     "iopub.status.busy": "2021-01-11T15:46:33.149334Z",
     "iopub.status.idle": "2021-01-11T15:46:33.228690Z",
     "shell.execute_reply": "2021-01-11T15:46:33.228342Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 200\n",
    "verbose = True\n",
    "lr = 0.01\n",
    "lambda_ = 0\n",
    "\n",
    "lr = LogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 4.2969 train accuracy: 0.498\n",
      "Epoch 20 \t cross entropy loss: 2.9803 train accuracy: 0.637\n",
      "Epoch 40 \t cross entropy loss: 2.2397 train accuracy: 0.706\n",
      "Epoch 60 \t cross entropy loss: 1.8942 train accuracy: 0.740\n",
      "Epoch 80 \t cross entropy loss: 1.6722 train accuracy: 0.764\n",
      "Epoch 100 \t cross entropy loss: 1.3735 train accuracy: 0.796\n",
      "Epoch 120 \t cross entropy loss: 1.1819 train accuracy: 0.824\n",
      "Epoch 140 \t cross entropy loss: 0.9254 train accuracy: 0.846\n",
      "Epoch 160 \t cross entropy loss: 0.7978 train accuracy: 0.863\n",
      "Epoch 180 \t cross entropy loss: 0.8682 train accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lr.fit(X_train, y_train)\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "F1 score: 0.7425742574257425\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Lexicon-Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect positive and negative sentiment words, and discard those that are not in the vocabulary of the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of negative words: 4814\n",
      "Negative words in vocab: 2986\n",
      "Original number of positive words: 2036\n",
      "Positive words in vocab: 1445\n"
     ]
    }
   ],
   "source": [
    "neg_word_file = \"opinion-lexicon-English/negative-words.txt\"\n",
    "pos_word_file = \"opinion-lexicon-English/positive-words.txt\"\n",
    "\n",
    "vocab = pickle.load(open(\"vocab.pkl\", \"rb\"))\n",
    "\n",
    "with open(neg_word_file, encoding=\"ISO-8859-1\") as f:\n",
    "    neg_words_orig = f.readlines()\n",
    "print(\"Original number of negative words:\", len(neg_words_orig))\n",
    "neg_words = []\n",
    "for word in neg_words_orig[31:]:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        neg_words.append(candidate)\n",
    "\n",
    "print(\"Negative words in vocab:\", len(neg_words))\n",
    "            \n",
    "with open(pos_word_file, encoding=\"ISO-8859-1\") as f:\n",
    "    pos_words_orig = f.readlines()\n",
    "print(\"Original number of positive words:\", len(pos_words_orig))\n",
    "pos_words = []\n",
    "for word in pos_words_orig[31:]:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        pos_words.append(candidate)\n",
    "\n",
    "print(\"Positive words in vocab:\", len(pos_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess accuracy on test set, as training set was used at least to generate vocabulary. Subtract \"points\" from the prediction for the counts all negative words in the document, and add \"points\" for the counts of all positive words. Predict $1$ for positive score, and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "n, d = X_test.shape\n",
    "\n",
    "scores = np.zeros(n)\n",
    "\n",
    "for word in neg_words:\n",
    "    scores -= X_test[:, vocab[word]]\n",
    "        \n",
    "for word in pos_words:\n",
    "    scores += X_test[:, vocab[word]]\n",
    "    \n",
    "y_pred = (scores > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "F1 score: 0.691542288557214\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
