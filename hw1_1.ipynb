{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:37:59.056674Z",
     "iopub.status.busy": "2021-01-11T14:37:59.056241Z",
     "iopub.status.idle": "2021-01-11T14:38:02.220365Z",
     "shell.execute_reply": "2021-01-11T14:38:02.220770Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.223317Z",
     "iopub.status.busy": "2021-01-11T14:38:02.222964Z",
     "iopub.status.idle": "2021-01-11T14:38:02.224598Z",
     "shell.execute_reply": "2021-01-11T14:38:02.224249Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_files = glob.glob(\"txt_sentoken/neg/*.txt\")\n",
    "pos_files = glob.glob(\"txt_sentoken/pos/*.txt\")\n",
    "files = neg_files + pos_files\n",
    "vocab = {}\n",
    "index = 0\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "            \n",
    "pickle.dump(vocab, open(\"vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word count feature vectors for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.228124Z",
     "iopub.status.busy": "2021-01-11T14:38:02.227760Z",
     "iopub.status.idle": "2021-01-11T14:38:02.228458Z",
     "shell.execute_reply": "2021-01-11T14:38:02.228781Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "n = len(files)\n",
    "n_neg = len(neg_files)\n",
    "n_pos = len(pos_files)\n",
    "\n",
    "X = np.zeros((n, vocab_size), dtype=np.int8)\n",
    "y = np.concatenate((np.repeat(0, n_neg), np.repeat(1, n_pos))).astype(np.int8)\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    with open(file) as f:\n",
    "        raw_txt = f.read()\n",
    "    txt_arr = raw_txt.split()\n",
    "    for token in txt_arr:\n",
    "        X[i, vocab[token]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=400)\n",
    "\n",
    "pickle.dump(X_train, open(\"X_train.pkl\", \"wb\"))\n",
    "pickle.dump(X_test, open(\"X_test.pkl\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"y_train.pkl\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"y_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.232171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.231797Z",
     "iopub.status.idle": "2021-01-11T14:38:02.855965Z",
     "shell.execute_reply": "2021-01-11T14:38:02.855580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1600\n",
      "Test size: 400\n",
      "Vocabulary size: 50920\n",
      "\n",
      "X_train shape: (1600, 50920)\n",
      "X_test shape: (400, 50920)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open(\"X_train.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"X_test.pkl\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y_train.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"y_test.pkl\", \"rb\"))\n",
    "\n",
    "vocab = pickle.load(open(\"vocab.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print()\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:38:02.865171Z",
     "iopub.status.busy": "2021-01-11T14:38:02.863077Z",
     "iopub.status.idle": "2021-01-11T14:38:02.866412Z",
     "shell.execute_reply": "2021-01-11T14:38:02.866744Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegressor():\n",
    "    \n",
    "    def __init__(self, lambda_, batch_size, epochs, lr, verbose):\n",
    "        self.lambda_ = lambda_\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n, d = X.shape\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        if not hasattr(self, 'weights'):\n",
    "            self.weights = np.random.normal(size=d+1)\n",
    "        checkpoint = self.epochs // 10\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            order = np.random.permutation(n)\n",
    "            num_batch = n // self.batch_size\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                indices = order[i:min(i + batch_size, n)]\n",
    "                X_batch = X_appended[indices]\n",
    "                y_batch = y[indices]\n",
    "                \n",
    "                self.gradient_step(X_batch, y_batch)\n",
    "                \n",
    "            # Compute training loss.\n",
    "            if verbose and (epoch % checkpoint == 0):\n",
    "                logits = np.dot(X_appended, self.weights)\n",
    "                probs = self.sigmoid(logits)\n",
    "                loss = (-(y * np.log(probs) + (1 - y) * np.log(1 - probs)).sum() + 0.5 * self.lambda_ * np.dot(self.weights, self.weights)) / n\n",
    "                \n",
    "                y_pred = (1 + np.sign(logits)) / 2\n",
    "                train_error = np.mean(np.abs(y - y_pred))\n",
    "                print(\"Epoch %d \\t cross entropy loss: %0.4f train error %0.3f\" % (epoch, loss, train_error))\n",
    "                \n",
    "                losses.append(loss)\n",
    "            \n",
    "                \n",
    "        self.fitted = True\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if not hasattr(self, 'fitted'):\n",
    "            raise NotFittedError(\"This MultinomialLogisticRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\")\n",
    "            \n",
    "        n = len(X)\n",
    "        X_appended = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        \n",
    "        return (1 + np.sign(np.dot(X_appended, self.weights))) / 2\n",
    "    \n",
    "    def gradient_step(self, X, y):\n",
    "        \n",
    "        # Use l1 regularization.\n",
    "        \n",
    "        n = len(X)\n",
    "        probs = self.sigmoid(np.dot(X, self.weights))\n",
    "        grad = (np.dot(X.T, (probs - y)) + self.lambda_ * self.weights) / n\n",
    "        self.weights = self.weights - self.lr * grad\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        Z_clipped = np.clip(Z, -10, 10)\n",
    "        return 1 / (1 + np.exp(-Z_clipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the code works on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:32.929921Z",
     "iopub.status.busy": "2021-01-11T15:46:32.929571Z",
     "iopub.status.idle": "2021-01-11T15:46:33.145605Z",
     "shell.execute_reply": "2021-01-11T15:46:33.145252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 0.0417 train error 0.013\n",
      "Epoch 10 \t cross entropy loss: 0.0023 train error 0.000\n",
      "Epoch 20 \t cross entropy loss: 0.0009 train error 0.000\n",
      "Epoch 30 \t cross entropy loss: 0.0007 train error 0.000\n",
      "Epoch 40 \t cross entropy loss: 0.0005 train error 0.000\n",
      "Epoch 50 \t cross entropy loss: 0.0005 train error 0.000\n",
      "Epoch 60 \t cross entropy loss: 0.0004 train error 0.000\n",
      "Epoch 70 \t cross entropy loss: 0.0003 train error 0.000\n",
      "Epoch 80 \t cross entropy loss: 0.0003 train error 0.000\n",
      "Epoch 90 \t cross entropy loss: 0.0003 train error 0.000\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "d = 10\n",
    "\n",
    "mu = 3 * np.ones(d)\n",
    "cov = np.eye(d)\n",
    "\n",
    "X_neg = np.random.multivariate_normal(-mu, cov, size = n // 2)\n",
    "y_neg = np.repeat(0, n // 2)\n",
    "X_pos = np.random.multivariate_normal(mu, cov, size = n // 2)\n",
    "y_pos = np.repeat(1, n // 2)\n",
    "\n",
    "X = np.concatenate((X_neg, X_pos))\n",
    "y = np.concatenate((y_neg, y_pos))\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "verbose = True\n",
    "lr = 0.03\n",
    "lambda_ = 0.001\n",
    "\n",
    "mlr = MultinomialLogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "mlr.fit(X_tr, y_tr)\n",
    "y_pred = mlr.predict(X_te)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with better hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:33.149677Z",
     "iopub.status.busy": "2021-01-11T15:46:33.149334Z",
     "iopub.status.idle": "2021-01-11T15:46:33.228690Z",
     "shell.execute_reply": "2021-01-11T15:46:33.228342Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 300\n",
    "verbose = True\n",
    "lr = 0.03\n",
    "lambda_ = 0\n",
    "\n",
    "mlr = MultinomialLogisticRegressor(lambda_=lambda_, batch_size=batch_size, epochs=epochs, lr=lr, verbose=verbose)\n",
    "\n",
    "# 1. Does the loss monotonically decrease? -> \n",
    "# - yes: increase learning rate until no. Move to 2.\n",
    "# - no: decrease learning rate.\n",
    "# 2. Does the model optimize?\n",
    "# - yes: Move to 3.\n",
    "# - no: Increase epochs.\n",
    "# 3. Is there a lare gap between train acc and val acc?\n",
    "# - yes: increase lambda. Move to 1.\n",
    "# - no: Move to 4.\n",
    "# Is the performance reasonable?\n",
    "# - yes: You're done!\n",
    "# - no: decrease lambda. Move to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t cross entropy loss: 4.3488 train error 0.466\n",
      "Epoch 30 \t cross entropy loss: 2.0924 train error 0.260\n",
      "Epoch 60 \t cross entropy loss: 2.7844 train error 0.324\n",
      "Epoch 90 \t cross entropy loss: 1.1108 train error 0.152\n",
      "Epoch 120 \t cross entropy loss: 0.4267 train error 0.073\n",
      "Epoch 150 \t cross entropy loss: 0.3226 train error 0.066\n",
      "Epoch 180 \t cross entropy loss: 0.1980 train error 0.037\n",
      "Epoch 210 \t cross entropy loss: 0.1065 train error 0.033\n",
      "Epoch 240 \t cross entropy loss: 0.1711 train error 0.049\n",
      "Epoch 270 \t cross entropy loss: 0.0664 train error 0.024\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcy0lEQVR4nO3deXST950u8OerxZJlyatsy9jGMmFHhM2AWdo0JAGT5rSdNKFN0pCGJJDezr3tTM9s99yeM9OZM6fnNtPO3DtpGpqQlISkSWimnduyJL0hTQAHYpawL2YzJsaWjW28r7/5Q7KxwYAwkt731ft8zvFBSLL8oAPPefnp935fUUqBiIj0y6J1ACIiujEWNRGRzrGoiYh0jkVNRKRzLGoiIp2zxeJFvV6v8vv9sXhpIqKEtGfPnnqlVPZIj8WkqP1+PyoqKmLx0kRECUlEzl3vMS59EBHpHIuaiEjnWNRERDrHoiYi0jkWNRGRzrGoiYh0jkVNRKRzuinqzp4+rP3oFHZU1msdhYhIV3RT1HarBWs/Oo03dlVpHYWISFd0U9RWi2DZNB+2Ha9DZ0+f1nGIiHRDN0UNAMsDeWjv7sOfTgS1jkJEpBu6Kur54zKR7rJjy6GLWkchItINXRW13WrBfVNy8cejtejq5fIHERGgs6IGgOXTfWjp7MXOygatoxAR6YLuinrReC88Dhs2H6rROgoRkS7orqgdNiuWTMnB+0dq0dvXr3UcIiLN6a6oAWB5wIfG9h7sOnNJ6yhERJrTZVHfNTEHyXYrlz+IiKDTok5OsuLuydnYergW/f1K6zhERJrSZVEDQFkgD8GWLuypatQ6ChGRpnRb1Esm5yDJZsHmgzz5hYjMLeKiFhGriOwTkd/HMtAAt8OGL07wYsuhGijF5Q8iMq9bOaL+HoCjsQoykrJAHj5v7sRn1c3x/LFERLoSUVGLSAGALwN4KbZxhrtvSi5sFuHuDyIytUiPqP8VwF8DuO4ZKCKyWkQqRKQiGIzO9Ls0lx0L7sjClkMXufxBRKZ106IWkQcA1Cml9tzoeUqptUqpEqVUSXZ2dtQC3j89D+ca2nG0piVqr0lEZCSRHFEvAvAVETkL4NcAlojI6zFNNcTSqbmwCLCFyx9EZFI3LWql1N8ppQqUUn4A3wTwgVLqWzFPFpbldmBecSY2c0Y1EZmUbvdRD7U8kIeTda2orGvVOgoRUdzdUlErpT5USj0QqzDXs2yaDwCXP4jInAxxRO1Lc2L22HRs4lmKRGRChihqILT8caTmMqoa2rWOQkQUV4Yp6rJAaPmDJ78QkdkYpqgLM12Ynp/G3R9EZDqGKWogdFS9/3wTapo7tI5CRBQ3hirq5YGB3R88qiYi8zBUUY/LdmNSrofLH0RkKoYqaiC0/PHp2UsItnRpHYWIKC4MV9TLp/ugFLD1MI+qicgcDFfUk3I9KPamcJ2aiEzDcEUtIigL+FB+ugGNbd1axyEiijnDFTUA3B/IQ1+/wvtHa7WOQkQUc4Ys6kB+Kgoykrn8QUSmYMiiFhGUTfNh+8l6tHT2aB2HiCimDFnUQGj3R3dfPz44Vqd1FCKimDJsUc8qzEBuqgObOfqUiBKcYYvaYhEsm+bDhyfq0N7dq3UcIqKYMWxRA6GzFDt7+vHh8aDWUYiIYsbQRT3Pn4nMlCTO/iCihGboorZZLVg2LRcfHK1FZ0+f1nGIiGLC0EUNAGWBPLR192H7yXqtoxARxYThi3rBuCykOm1c/iCihGX4ok6yWXDv1Fz88Wgtevr6tY5DRBR1hi9qIHSF8uaOHpSfatA6ChFR1CVEUX9hghcpSVYufxBRQkqIonbarbh7cg7eO3wRff1K6zhERFGVEEUNhJY/Gtq6sfvMJa2jEBFFVcIU9ZcmZcNpt2DLoRqtoxARRVXCFHWKw4a7JmZjy+GL6OfyBxElkIQpaiC0/FF7uQv7zjdpHYWIKGoSqqiXTMmB3Spc/iCihJJQRZ3qtGPxeC82H7oIpbj8QUSJIaGKGggtf1Q3duDw55e1jkJEFBUJV9T3Tc2F1SLYdJDLH0SUGBKuqDNSklA6LhNbuPxBRAki4YoaCC1/nK5vw4naVq2jEBHdtoQs6qXTciECbObuDyJKADctahFxishuEflMRA6LyD/EI9jtyPE4MbcotPxBRGR0kRxRdwFYopSaAWAmgDIRKY1pqigoC/hw7GILztS3aR2FiOi23LSoVcjAYq89/KX7T+nKAj4AXP4gIuOLaI1aRKwish9AHYD3lVK7YpoqCsakJ2NGYTqXP4jI8CIqaqVUn1JqJoACAPNEJHD1c0RktYhUiEhFMBiMcszRWR7w4UB1M6ob27WOQkQ0are060Mp1QTgQwBlIzy2VilVopQqyc7Ojk6627Q8vPzBo2oiMrJIdn1ki0h6+HYygHsBHItxrqgoykrB1LxUXqKLiAwtkiPqPADbROQAgE8RWqP+fWxjRc/ygA97zjWi9nKn1lGIiEYlkl0fB5RSs5RSdyqlAkqpH8UjWLQsnx5a/th6mEfVRGRMCXlm4lDjczwYn+PG5oMsaiIypoQvaiC0/LHrTAMaWru0jkJEdMtMUdRlAR/6FfD+kVqtoxAR3TJTFPXUvFSMzXQZavdHb18/Tta2aB2DiHTAFEUtIlge8GFHZT2a23u0jnNTnT19+M6GvbjvZx9hR2W91nGISGOmKGogtPzR26/wx6P6Xv5o7+7F07+qwPtHauFKsuKlj09rHYmINGaaop5ZmI4xaU5dL380d/Tg8Zd3Y+epevzLwzOw+ovjsO14EKeCvAACkZmZpqhFBMsCPnx0MojWrl6t41yjvrULj6z9BAeqm/Dzx2bj63MK8Nj8IiRZLXh1x1mt4xGRhkxT1EDoEl3dvf3YdqxO6yjD1DR3YMWL5Thd34qXnpiLskAeACDb48BXZo7Bxj3VhlhbJ6LYMFVRzynKgNft0NWQpnMNbXjohXIEL3fhtafm466JwwdaPbnIj46ePvz60yqNEhKR1kxV1FaLYNm0XGw7XofOnj6t4+BEbQse/kU52rt78ebqUsz1Z17znGlj0lA6LhO/2nkWvX39GqQkIq2ZqqiB0PJHe3cf/nRC25nZB6qbsOLFcogAb69ZgEB+2nWfu2pRMT5v7sTWw/resUJEsWG6op4/LhPpLjs2H9TuEl27Tjfg0V/ugsdpwztrFmJCrueGz79nSi7GZrqwbseZOCUkIj0xXVHbrRbcNyUX//9oHbp647/88eHxOqxctxu+NCfeWbMQY7NcN/0eq0Xw7YV+7DnXiP3nm2Ifkoh0xXRFDQD3T89DS1cvdlY2xPXnbjpYg2fWV2B8jhtvrS6FL80Z8fc+XFIAt8OGV3hUTWQ6pizqheOz4HHY4nqF8ncqzuPP39iLGQXpeHN1KbLcjlv6fo/TjhUlhfjDgRpcbOZFEIjMxJRF7bBZcc+UHLx/pDYuOyle3XEGf7XxABaN92L9U/OQ6rSP6nW+vdCPPqXw2idnoxuQiHTNlEUNAGWBPDS292DXmUsx+xlKKTy/rRJ///+OYNm0XLz0RAlcSbZRv97YLBeWTs3FG7uq0NGt/fZCIooP0xb1XROzkWy3xmz5QymFH285hp9sPY4HZ+Xj+Udnw2Gz3vbrrlpUjMb2Hvx2/4UopCQiIzBtUScnWXH35GxsPVyL/n4V1dfu71f4X789hBf/dBqPlxbhuYdnwGaNzls9rzgT08akYt32M1AqurmJSJ9MW9RAaPkj2NKFPVWNUXvN3r5+/OCdz7BhVxW+86U78KOvToPFIlF7fRHBqkXFOFnXiu2cVU1kCqYu6iWTc5Bks2BTlE5+6ertw3/bsBf/se8C/mrZJPxN2WSIRK+kBzwwIw9etwPrtnOrHpEZmLqo3Q4bvjghG1sPXbztZYT27l489WoF3jtSix99dRq+e/f4KKW8lsNmxeOlRZxVTWQSpi5qIHSF8s+bO/FZdfOoX2PowP/nHp6BlQv80Qt4HY+VjkWS1cITYIhMwPRFfe+UXNgsMurdH0MH/j//6Gw8NKcgyglH5nU78NWZY/CbPRfQ1N4dl59JRNowfVGnuexYON6LLaNY/rh64P/y6XkxSjmyJxcVh2dVn4/rzyWi+DJ9UQOh5Y9zDe04WtMS8fcMHfi/ftW1A//jYeqYVCwYl4Vf7TyLHs6qJkpYLGoAS6fmwiLAlgiXP4YO/H/jmVLMK7524H+8rFpcjJrmTmw9rJ+r1hBRdLGoAWS5HZhXnBnRFcoHBv4DoYH/0wuuP/A/HpZMzkFRlotb9YgSGIs6bHkgDyfrWlFZd/3lj4GB/26HDRufvfnA/3gYmFW9t6oJ+6J44g4R6QeLOqws4AMAbD448lH1wMD/3FQHNj4b2cD/eHm4pBAehw2v7DirdRQiigEWdVhuqhNzijJGXP4YOvD/7TULbmngfzy4HTZ8Y24hNh2sQU1zh9ZxiCjKWNRDLA/4cKTmMqoa2gfvGzrw/41nbn3gf7w8sdCPfqXwWvk5raMQUZSxqIdYNi28/BHe/XH1wP+05NEN/I+HwkwXlk714Y3dnFVNlGhY1EMUZrowPT8Nmw9dHBz4v3Tq7Q/8j5dVi4vR1N6D/9jHWdVEiYRFfZWygA/7zzfhJ1uP489m5ePnj0Vn4H88zPVnIJCfinU7OKuaKJGwqK/ywJ15cNoteLy0CP8SxYH/8TAwq7qyrhUfn+SsaqJEYZwWipOirBTs++FS/OPXAlEd+B8vX74zD9keB9Zxqh5RwrhpUYtIoYhsE5GjInJYRL4Xj2BaSk4yxlLHSAZmVX94PIjKOs6qJkoEkRxR9wL4gVJqCoBSAN8VkamxjUW349H5Y5Fks+DVnTyqJkoENy1qpVSNUmpv+HYLgKMA8mMdjEbP63bga5xVTZQwbmmNWkT8AGYB2DXCY6tFpEJEKoLBYJTi0WitWhyaVf3mbs6qJjK6iItaRNwAfgPg+0qpy1c/rpRaq5QqUUqVZGfHfzYzDTfZl4pF47OwvpyzqomMLqKiFhE7QiW9QSn1bmwjUbSsWhSaVb0lgvGtRKRfkez6EAAvAziqlPpp7CNRtNw9KQf+LBe36hEZXCRH1IsAPA5giYjsD3/dH+NcFAUWi+DJRcXYV9WEvZxVTWRYkez62K6UEqXUnUqpmeGvTfEIR7fvoTkF8Dg5q5rIyHhmYoJLcdjwTc6qJjI0FrUJrFzgh1IK6zmrmsiQWNQmUJjpwrJpPryxi7OqiYyIRW0SqxYXo7mjB+/uq9Y6ChHdIha1SZQUZWB6fhrWbT+D/n7OqiYyEha1SYgIVi3241SwDR9XclY1kZGwqE3ky9PHhGZVb+cJMERGwqI2kSSbBStLi/CnE0FU1rVoHYeIIsSiNplH54+Fw2bhCTBEBsKiNpkstwN/Nisfv9lbzVnVRAbBojahJxcVo7Onn7OqiQyCRW1Ck3weLB7vxa92clY1kRGwqE1q1WI/Ll7uxGbOqibSPRa1SX1pYg6KvSncqkdkACxqkwrNqvZj/3nOqibSOxa1iX19dmhWNY+qifSNRW1iKQ4bHpk3FpsPXcTnTZxVTaRXLGqTW7mgiLOqiXSORW1yBRkulAV8eHN3Fdq7e7WOQ0QjYFETVi0Kz6ree0HrKEQ0AhY1YU5RBu4sSMMrOzirmkiPWNQEEcFTi4txKtiGj04GtY5DRFdhURMAYHkgD7mpDqzjVD0i3WFRE4DwrOoFfnx0IoiTtZxVTaQnLGoa9Mi88KzqnWe1jkJEQ7CoaVBmShIenJ2Pd/dWo7GNs6qJ9IJFTcMMzqr+tErrKEQUxqKmYSbmevCFCV6s33mOs6qJdIJFTddYtaiYs6qJdIRFTde4a2I2xnlT8PL2M1CKJ8AQaY1FTdcYmFX92fkm7K1q0joOkemxqGlED84uQKrThnU7OKuaSGssahrRwKzqLYcu4gJnVRNpikVN17VyoR8AsL78rKY5iMyORU3XlZ+ejLKADxs+qcKB6iat4xCZFouabuhvyyYjI8WOR3+5C+WnGrSOQ2RKLGq6ocJMF95ZsxB5aU488cpu/PFIrdaRiEznpkUtIutEpE5EDsUjEOmPL82Jt9cswBSfB2te34Pf7uOVYIjiKZIj6lcBlMU4B+lcRkoSNjxTinn+TPzF2/vxGj9gJIqbmxa1UuojAJfikIV0zu2w4ZUn5+LeKbn44e8O498/OMkzF4niIGpr1CKyWkQqRKQiGOTlnBKV027FC4/NxoOz8vHceyfwz5uOsqyJYswWrRdSSq0FsBYASkpK+C83gdmsFjz38Ax4nDb88uMzuNzRi39+cDqsFtE6GlFCilpRk7lYLIK//8o0pCXb8X8+qERLVw9+9o2ZcNisWkcjSjgsaho1EcFfLp2E1GQ7/ukPR9HSWYEXH58DVxL/WhFFUyTb894EUA5gkohUi8hTsY9FRvL0F8bhf3/9TuyorMfjL+9Gc3uP1pGIEkokuz4eUUrlKaXsSqkCpdTL8QhGxrJibiF+/thsHKxuxjfWliPY0qV1JKKEwTMTKWrKAnl4+dslONfQjod/sRPVje1aRyJKCCxqiqovTMjG60/Px6W2bjz0Qjkq61q0jkRkeCxqiro5RRl4a80C9PYrrHjxExysbtY6EpGhsagpJqbkpWLjswvgSrLikV9+gl2nOXmPaLRY1BQzfm8KNj67EL40J1au240PjnHyHtFosKgppgYm703yebB6/R78bj8n7xHdKhY1xVxmShI2PD0fJf4MfP+t/Xjtk3NaRyIyFBY1xYXHacerT87DPZNz8MPfHsLz2yo5zIkoQixqihun3YoXvjUHX5s5Bj/Zehw/3nyMZU0UAQ5loLiyWy346YqZSE2248WPTuNyZw/+6WucvEd0IyxqijuLRfAP4cl7//eDSlzu6MXPvjETSTb+B49oJCxq0oSI4AdLJyFtYPJeVy9+8a3ZnLxHNAIewpCmBibvbT8ZxMqXd6O5g5P3iK7GoibNrZhbiH9/dDY+q27CI2s/4eQ9oquwqEkX7p+eh5eemIsz9W1Y8WI5LjR1aB2JSDdY1KQbd03MxutPz0NDaxceemEnKutatY5EpAssatKVOUWZeGvNAvT0Kax4sRyHLnDyHhGLmnRnSl4q3nl2AZLtVjyylpP3iFjUpEvF3hRs/M4C5KQ6sHLdbmw7Vqd1JCLNsKhJt/LSkvH2mgWYmOvBM+sr8Py2Snx8Mojzl9rR189Tz8k8eHYB6VqW24E3npmPZ1/fg59sPT54v90qKMx0oTgrBUVZKfB7XfBnpcCflYIx6U7YrDwGocTBoibd8zjteP2p+ahr6cKZ+jaca2jD2YZ2nK0P/Vp+ugHt3X2Dz7dbBYUZLhRlueD3hsq7KMuFYm8K8tOTWeJkOCxqMgQRQW6qE7mpTpSOyxr2mFIKwZauIeUd/qpvx+4zl9A2pMRtltCReFHWwBG4C0XhMi/ISIadJU46xKImwxMR5KQ6kZPqxLzizGGPKaUQbO3CuWElHrpdcbYRrV29g8+1WgQFGclXCjwrBcXe0NF4QYaLQ6NIMyxqSmgighyPEzkeJ+b6ry3xhrbuwSWUgSI/19COveca0XJVieenJ6Moy4U7st2Y5PNgYq4bE3I9SHXa4/3HIpNhUZNpiQi8bge8bgdKRijxS23dgwV+rqENZ8K33644P2xNPC/NiYm5oeIO/erBhFw3JwFS1PBvEtEIRARZbgey3A7MKcoY9lh/v8KFpg6cqG3BidrW8K8tKD/dgO7e/vD3AwUZyZgULu6Br3HZKXDarVr8kcjAWNREt8gS/kCyMNOFe6bkDt7f169Qdakdxy+2DJb3idoWfHg8iN7wvm+LAH5vCibmeDAxvHwyKdcDvzeFH2TSdbGoiaLEahEUe0MfQJYFfIP3d/f242xDW6i4L145Cn/vyEUMnLdjtwrGed2YEC7uCbkeTPJ5MDbTxcuUEYuaKNaSbJbBpQ/ceeX+zp4+nAq2XllCudiCz6qb8PsDNYPPcdgsGJ8ztLzdmJDjQX56MiwscNNgURNpxGm3YtqYNEwbkzbs/rauXlTWteJ4bQtO1rbgeG0ryk834N19Fwafk5JkRWFmaNtgQUZy+Ct0uzDDhdRkG0RY5ImCRU2kMykOG2YUpmNGYfqw+5s7enByyAeY1Y3tqG5sR/mp+mEn9QCAx2FD/pDyNnqRK6XQ3NGD+tZu1Ld2ob61Cw2Dt7vR2dOHgozk8N730MlMmSlJhvoz3giLmsgg0pLtKPFnjriVsLmjB9WNHeHy7hhyW79F3tPXj8a2bgTDZdswpICDw4o4dLt3hEFcFgEyU5LgsFnxu/0dGPoUj9MWOnnJGzqByT9kJozRSpxFTWRwIoJ0VxLSXUkI5Kdd8/jNivyT0w3DztAERl/kHd19qG/tGla0DeEiHijdgVJubB/5QsZJNguy3Q5kuZOQm+rE1LxUeD2O8J73JHjDj3ndDmS4kgY/bO3u7cf5xvbQnvf6Kycw7T/fiD8c+DyCEg/d1mOJi1LRHxdZUlKiKioqov66RBR91y/yK7+/usjdDhsKMpKRk+pES2fPYCm3X3XkPsDjtI1YtFluB7LdSchyXylityP6R/PXK/GzDW240Hj9I/Hi8CiBeJS4iOxRSpWM+BiLmohuRCmFyx29OD9Y4lfKvK6lE6lO+2DxDpRw9pAyzkxJ0vVJPrda4qH5L8NLvNibggyX/bZKnEVNRDQKt1riU3ypeGtN6agK+0ZFHdEatYiUAfg3AFYALymlfnzLKYiIDCbJZsEd2W7cke2+5rGRSrynrz8mSyM3LWoRsQJ4HsB9AKoBfCoi/6mUOhL1NEREBnGjEo+2SIYLzANQqZQ6rZTqBvBrAF+NbSwiIhoQSVHnAzg/5PfV4fuGEZHVIlIhIhXBYDBa+YiITC+Soh5pweWaTyCVUmuVUiVKqZLs7OzbT0ZERAAiK+pqAIVDfl8A4PPYxCEioqtFUtSfApggIsUikgTgmwD+M7axiIhowE13fSilekXkzwFsRWh73jql1OGYJyMiIgAR7qNWSm0CsCnGWYiIaAS89g8Rkc7F5BRyEQkCODfKb/cCqI9iHCPjezEc34/h+H5ckQjvRZFSasQtczEp6tshIhXXO9/dbPheDMf3Yzi+H1ck+nvBpQ8iIp1jURMR6Zwei3qt1gF0hO/FcHw/huP7cUVCvxe6W6MmIqLh9HhETUREQ7CoiYh0TjdFLSJlInJcRCpF5G+1zqMlESkUkW0iclREDovI97TOpDURsYrIPhH5vdZZtCYi6SKyUUSOhf+OLNA6k5ZE5C/C/04OicibIuLUOlO06aKoh1xFZjmAqQAeEZGp2qbSVC+AHyilpgAoBfBdk78fAPA9AEe1DqET/wZgi1JqMoAZMPH7IiL5AP4HgBKlVACheUTf1DZV9OmiqMGryAyjlKpRSu0N325B6B/iNRdrMAsRKQDwZQAvaZ1FayKSCuCLAF4GAKVUt1KqSdNQ2rMBSBYRGwAXEnAMs16KOqKryJiRiPgBzAKwS+MoWvpXAH8NoF/jHHowDkAQwCvhpaCXRCRF61BaUUpdAPAcgCoANQCalVLvaZsq+vRS1BFdRcZsRMQN4DcAvq+Uuqx1Hi2IyAMA6pRSe7TOohM2ALMBvKCUmgWgDYBpP9MRkQyE/vddDGAMgBQR+Za2qaJPL0XNq8hcRUTsCJX0BqXUu1rn0dAiAF8RkbMILYktEZHXtY2kqWoA1Uqpgf9hbUSouM3qXgBnlFJBpVQPgHcBLNQ4U9Tppah5FZkhREQQWoM8qpT6qdZ5tKSU+julVIFSyo/Q34sPlFIJd8QUKaXURQDnRWRS+K57ABzRMJLWqgCUiogr/O/mHiTgh6sRXTgg1ngVmWssAvA4gIMisj983/8MX8CB6L8D2BA+qDkN4EmN82hGKbVLRDYC2IvQbql9SMDTyXkKORGRzull6YOIiK6DRU1EpHMsaiIinWNRExHpHIuaiEjnWNRERDrHoiYi0rn/AuWLKn+CSksHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    losses = mlr.fit(X_train, y_train)\n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  0.245\n",
      "Accuracy: 0.755\n",
      "F1 score: 0.7699530516431925\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlr.predict(X_test)\n",
    "\n",
    "test_error = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "print(\"Test error: \", test_error)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T15:46:33.230962Z",
     "iopub.status.busy": "2021-01-11T15:46:33.230618Z",
     "iopub.status.idle": "2021-01-11T15:46:33.233062Z",
     "shell.execute_reply": "2021-01-11T15:46:33.233332Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(mlr, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Lexicon-Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of negative words: 4783\n",
      "Negative words in vocab: 3124\n",
      "Original number of positive words: 2006\n",
      "Positive words in vocab: 1489\n"
     ]
    }
   ],
   "source": [
    "neg_word_file = \"opinion-lexicon-English/neg_words.txt\"\n",
    "pos_word_file = \"opinion-lexicon-English/pos_words.txt\"\n",
    "\n",
    "vocab = pickle.load(open(\"vocab.pkl\", \"rb\"))\n",
    "\n",
    "with open(neg_word_file) as f:\n",
    "    neg_words_orig = f.readlines()\n",
    "print(\"Original number of negative words:\", len(neg_words_orig))\n",
    "neg_words = []\n",
    "for word in neg_words_orig:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        neg_words.append(candidate)\n",
    "\n",
    "print(\"Negative words in vocab:\", len(neg_words))\n",
    "            \n",
    "with open(pos_word_file) as f:\n",
    "    pos_words_orig = f.readlines()\n",
    "print(\"Original number of positive words:\", len(pos_words_orig))\n",
    "pos_words = []\n",
    "for word in pos_words_orig:\n",
    "    candidate = word[0:-1] \n",
    "\n",
    "    # Ignore words that are not in the vocab.\n",
    "    if candidate in vocab:\n",
    "        pos_words.append(candidate)\n",
    "\n",
    "print(\"Positive words in vocab:\", len(pos_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(\"X_train.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"X_test.pkl\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y_train.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"y_test.pkl\", \"rb\"))\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "n, d = X.shape\n",
    "\n",
    "scores = np.zeros(n)\n",
    "\n",
    "for word in neg_words:\n",
    "    scores -= X[:, vocab[word]]\n",
    "        \n",
    "for word in pos_words:\n",
    "    scores += X[:, vocab[word]]\n",
    "    \n",
    "y_pred = (scores > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.703\n",
      "F1 score: 0.6847133757961783\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"F1 score:\", f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
